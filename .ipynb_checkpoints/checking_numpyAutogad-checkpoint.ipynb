{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opponent-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.scipy.special import logsumexp\n",
    "from os.path import dirname, join\n",
    "from autograd.misc.optimizers import adam\n",
    "from misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unique-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, covs, signals, [A,dt,C,D] = load_data(periods=40,ppp=500,itraj=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supported-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_params():\n",
    "    return {'init hiddens': np.array([[1.,0.]]),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "functional-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(params, inputs):\n",
    "    def update_rnn(input, hiddens):\n",
    "        return np.dot(A,hiddens)*dt\n",
    "\n",
    "    def get_output(hiddens):\n",
    "        output = np.einsum('ij,bj->bi',C, hiddens)*dt\n",
    "        return output \n",
    "\n",
    "    num_sequences = signals.shape[0]\n",
    "    hiddens = np.repeat(params['init hiddens'], num_sequences, axis=0)\n",
    "    output = [hiddens_to_output_probs(hiddens)]\n",
    "\n",
    "    for input in inputs:  # Iterate over time steps.\n",
    "        hiddens = update_rnn(input, hiddens)\n",
    "        output.append(hiddens_to_output_probs(hiddens))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "marked-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = create_rnn_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_mse(params,inputs,targets):\n",
    "    num_time_steps = len(inputs)\n",
    "    for t in range(num_time_steps):\n",
    "        loglik += np.sum(logprobs[t] * targets[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss(params, iter):\n",
    "    return -rnn_log_likelihood(params, train_inputs, train_inputs)\n",
    "\n",
    "def callback(weights, iter, gradient):\n",
    "    if iter % 10 == 0:\n",
    "        print(\"Iteration\", iter, \"Train loss:\", training_loss(weights, 0))\n",
    "        print_training_prediction(weights)\n",
    "\n",
    "# Build gradient of loss function using autograd.\n",
    "training_loss_grad = grad(training_loss)\n",
    "\n",
    "print(\"Training RNN...\")\n",
    "trained_params = adam(training_loss_grad, init_params, step_size=0.1,\n",
    "                      num_iters=1000, callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-testimony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-invasion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inappropriate-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_predict(weights, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-access",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-healthcare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.einsum('ij,bj->bi',self.C, sts)*self.dt\n",
    "\n",
    "xicov = tf.einsum('bij,jk->bik',cov,tf.transpose(self.C)) + tf.transpose(self.D)\n",
    "A_minus_xiC = A - tf.einsum('bij,jk->bik',xicov,self.C)\n",
    "\n",
    "dx = tf.einsum('bij,bj->bi',A_minus_xiC, sts)*self.dt + tf.einsum('bij,bj->bi', xicov, dy)\n",
    "x = sts + dx\n",
    "\n",
    "cov_dt = tf.einsum('ij,bjk->bik',A,cov) + tf.einsum('bij,jk->bik',cov, tf.transpose(A)) + self.D - tf.einsum('bij,bjk->bik',xicov, tf.transpose(xicov, perm=[0,2,1]))\n",
    "new_cov = cov + cov_dt*self.dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Helper functions #################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1.0)   # Output ranges from 0 to 1.\n",
    "\n",
    "def concat_and_multiply(weights, *args):\n",
    "    cat_state = np.hstack(args + (np.ones((args[0].shape[0], 1)),))\n",
    "    return np.dot(cat_state, weights)\n",
    "\n",
    "\n",
    "### Define recurrent neural net #######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rnn_log_likelihood(params, inputs, targets):\n",
    "    logprobs = rnn_predict(params, inputs)\n",
    "    loglik = 0.0\n",
    "    num_time_steps, num_examples, _ = inputs.shape\n",
    "    for t in range(num_time_steps):\n",
    "        loglik += np.sum(logprobs[t] * targets[t])\n",
    "    return loglik / (num_time_steps * num_examples)\n",
    "\n",
    "\n",
    "### Dataset setup ##################\n",
    "\n",
    "def string_to_one_hot(string, maxchar):\n",
    "    \"\"\"Converts an ASCII string to a one-of-k encoding.\"\"\"\n",
    "    ascii = np.array([ord(c) for c in string]).T\n",
    "    return np.array(ascii[:,None] == np.arange(maxchar)[None, :], dtype=int)\n",
    "\n",
    "def one_hot_to_string(one_hot_matrix):\n",
    "    return \"\".join([chr(np.argmax(c)) for c in one_hot_matrix])\n",
    "\n",
    "def build_dataset(filename, sequence_length, alphabet_size, max_lines=-1):\n",
    "    \"\"\"Loads a text file, and turns each line into an encoded sequence.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    content = content[:max_lines]\n",
    "    content = [line for line in content if len(line) > 2]   # Remove blank lines\n",
    "    seqs = np.zeros((sequence_length, len(content), alphabet_size))\n",
    "    for ix, line in enumerate(content):\n",
    "        padded_line = (line + \" \" * sequence_length)[:sequence_length]\n",
    "        seqs[:, ix, :] = string_to_one_hot(padded_line, alphabet_size)\n",
    "    return seqs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_chars = 128\n",
    "\n",
    "    # Learn to predict our own source code.\n",
    "    text_filename = join(dirname(__file__), 'rnn.py')\n",
    "    train_inputs = build_dataset(text_filename, sequence_length=30,\n",
    "                                 alphabet_size=num_chars, max_lines=60)\n",
    "\n",
    "    init_params = create_rnn_params(input_size=128, output_size=128,\n",
    "                                    state_size=40, param_scale=0.01)\n",
    "\n",
    "    def print_training_prediction(weights):\n",
    "        print(\"Training text                         Predicted text\")\n",
    "        logprobs = np.asarray(rnn_predict(weights, train_inputs))\n",
    "        for t in range(logprobs.shape[1]):\n",
    "            training_text  = one_hot_to_string(train_inputs[:,t,:])\n",
    "            predicted_text = one_hot_to_string(logprobs[:,t,:])\n",
    "            print(training_text.replace('\\n', ' ') + \"|\" +\n",
    "                  predicted_text.replace('\\n', ' '))\n",
    "\n",
    "    def training_loss(params, iter):\n",
    "        return -rnn_log_likelihood(params, train_inputs, train_inputs)\n",
    "\n",
    "    def callback(weights, iter, gradient):\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Iteration\", iter, \"Train loss:\", training_loss(weights, 0))\n",
    "            print_training_prediction(weights)\n",
    "\n",
    "    # Build gradient of loss function using autograd.\n",
    "    training_loss_grad = grad(training_loss)\n",
    "\n",
    "    print(\"Training RNN...\")\n",
    "    trained_params = adam(training_loss_grad, init_params, step_size=0.1,\n",
    "                          num_iters=1000, callback=callback)\n",
    "\n",
    "    print()\n",
    "    print(\"Generating text from RNN...\")\n",
    "    num_letters = 30\n",
    "    for t in range(20):\n",
    "        text = \"\"\n",
    "        for i in range(num_letters):\n",
    "            seqs = string_to_one_hot(text, num_chars)[:, np.newaxis, :]\n",
    "            logprobs = rnn_predict(trained_params, seqs)[-1].ravel()\n",
    "            text += chr(npr.choice(len(logprobs), p=np.exp(logprobs)))\n",
    "        print(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
